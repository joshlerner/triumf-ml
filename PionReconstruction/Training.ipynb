{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e2698c-8b28-4191-86aa-db41cd73329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9464f8b-7428-4fcb-8dde-92f72d25a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import uproot as ur\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973a7b67-3a1a-4949-9be5-6e79635a1dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"6\"\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "#RUN BEFORE#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83399097-1891-48d2-a523-897113f197c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshualerner/projects/src/hls4ml/hls4ml/converters/__init__.py:25: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to import handlers from core.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from convolution.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from reshape.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from pooling.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from merge.py: No module named 'torch'.\n"
     ]
    }
   ],
   "source": [
    "#RUN AFTER#\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow.keras as keras\n",
    "K = keras.backend\n",
    "\n",
    "from PionReconstruction.util.Models import *\n",
    "from PionReconstruction.util.Generators import *\n",
    "from PionReconstruction.util.Plotting import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762532cb-f6d9-4dc7-b2e3-f904d7c3abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = '/fast_scratch_1/atlas_images/v01-45/'\n",
    "\n",
    "cell_geo_path = data_path + 'cell_geo.root'\n",
    "\n",
    "out_path = '/fast_scratch_1/jlerner/data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f47e42db-533a-4f9c-83aa-5415d4c1379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare a sample for comparison and normalization if necessary\n",
    "\n",
    "# with ur.open(data_path + 'pi0/user.angerami.24559740.OutputStream._000232.root') as file:\n",
    "#     pi0_data = file['EventTree'].arrays(library='ak')\n",
    "    \n",
    "# with ur.open(data_path + 'pipm/user.angerami.24559744.OutputStream._000232.root') as file:\n",
    "#     pipm_data = file['EventTree'].arrays(library='ak')\n",
    "    \n",
    "# orig_pred = np.concatenate((ak.flatten(pipm_data['cluster_E']), \n",
    "#                             ak.flatten(pi0_data['cluster_E']))).to_numpy()\n",
    "\n",
    "# orig_target = np.concatenate((ak.flatten(pipm_data['cluster_ENG_CALIB_TOT']), \n",
    "#                               ak.flatten(pi0_data['cluster_ENG_CALIB_TOT']))).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f6c24bb-3f8a-4278-93a6-9e33a5f1ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 'log'\n",
    "vmax = 128\n",
    "\n",
    "if norm == 'std':\n",
    "    scaler = StandardScaler()\n",
    "elif norm == 'max':\n",
    "    scaler = 2000\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "if isinstance(scaler, type(StandardScaler())):\n",
    "    sample = orig_target.reshape(-1, 1)\n",
    "    scaler.fit(sample)\n",
    "\n",
    "normalizer = (norm, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a421617d-662f-4a5c-a560-499b0267ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = False\n",
    "train = True\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd5aae87-2505-4b09-9a0f-fc19b0e39c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 11:38:08.153520: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-08-05 11:38:08.153621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9119 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:b1:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "data (InputLayer)               [(None, 128, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vertex (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "garnet (GarNetStack)            (None, 16)           2976        data[0][0]                       \n",
      "                                                                 vertex[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "energy (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 17)           0           garnet[0][0]                     \n",
      "                                                                 energy[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           288         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8)            136         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (None, 2)            18          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "regression (Dense)              (None, 1)            9           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,427\n",
      "Trainable params: 3,427\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "quantize = True\n",
    "data_format = 'xne'\n",
    "\n",
    "q = ''\n",
    "if quantize:\n",
    "    q = 'q'\n",
    "\n",
    "if train:\n",
    "    K.clear_session()\n",
    "    model = GarNetModel(quantize=quantize, vmax=vmax, input_format=data_format)\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "092d59de-a158-4660-9ec1-e63bb01b86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_val_split = 0.8\n",
    "batch_size = 64\n",
    "\n",
    "pi0_list = [[data_path + f'pi0/user.angerami.24559740.OutputStream._000{i:03d}.root', 1] \n",
    "            for i in list(range(11, 113)) + list(range(116, 432))]\n",
    "pipm_list = [[data_path + f'pipm/user.angerami.24559744.OutputStream._000{i:03d}.root', 0] \n",
    "             for i in list(range(11, 113)) + list(range(116, 432))]\n",
    "\n",
    "np.random.shuffle(pi0_list)\n",
    "np.random.shuffle(pipm_list)\n",
    "\n",
    "train_start = 0\n",
    "train_end = train_start + int(train_val_split*len(pi0_list))\n",
    "val_start = train_end\n",
    "val_end = len(pi0_list)\n",
    "train_file_list = (pi0_list[train_start:train_end], pipm_list[train_start:train_end])\n",
    "val_file_list = (pi0_list[val_start:val_end], pipm_list[val_start:val_end])\n",
    "\n",
    "test_file_list = ([[data_path + f'pi0/user.angerami.24559740.OutputStream._000{i:03d}.root', 1] for i in range(432, 464)],\n",
    "                  [[data_path + f'pipm/user.angerami.24559744.OutputStream._000{i:03d}.root', 0] for i in range(432, 464)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3490dc2f-c1c9-4623-81dd-aacfa247d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = f'garnet_{normalizer[0]}_{vmax}'\n",
    "\n",
    "filterfunction = None    \n",
    "#def filterfunction(data):\n",
    "#    return data[1][-1] <= np.log(100)/10\n",
    "\n",
    "train_generator = garnetDataGenerator(train_file_list, \n",
    "                                      cell_geo_path, \n",
    "                                      batch_size,\n",
    "                                      normalizer=normalizer,\n",
    "                                      name=name,\n",
    "                                      vmax=vmax,\n",
    "                                      labeled=True,\n",
    "                                      preprocess=preprocess, \n",
    "                                      output_dir=out_path + 'train/',\n",
    "                                      data_format=data_format,\n",
    "                                      filterfunc=filterfunction)\n",
    "\n",
    "if preprocess: cell_geo_path = train_generator.geo_dict\n",
    "\n",
    "validation_generator = garnetDataGenerator(val_file_list, \n",
    "                                           cell_geo_path,\n",
    "                                           int(batch_size*(1 - train_val_split)/train_val_split),\n",
    "                                           normalizer=normalizer,\n",
    "                                           name=name,\n",
    "                                           vmax=vmax,\n",
    "                                           labeled=True, \n",
    "                                           preprocess=preprocess, \n",
    "                                           output_dir=out_path + 'val/',\n",
    "                                           data_format=data_format,\n",
    "                                           filterfunc=filterfunction)\n",
    "\n",
    "test_generator = garnetDataGenerator(test_file_list,\n",
    "                                     cell_geo_path,\n",
    "                                     batch_size=20000,\n",
    "                                     normalizer=normalizer,\n",
    "                                     name=name,\n",
    "                                     vmax=vmax,\n",
    "                                     labeled=True,\n",
    "                                     preprocess=preprocess,\n",
    "                                     output_dir=out_path + 'test/',\n",
    "                                     data_format=data_format,\n",
    "                                     filterfunc=filterfunction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b967b6e-4753-496b-b86f-a6d7e7c90db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 11:38:12.434692: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: [==================================================] 1000/1000\n",
      "30s - loss: 0.0877 - val loss: 0.0219\n",
      "Epoch 2/400: [==================================================] 1000/1000\n",
      "18s - loss: 0.0180 - val loss: 0.0111\n",
      "Epoch 3/400: [==================================================] 1000/1000\n",
      "21s - loss: 0.0131 - val loss: 0.0100\n",
      "Epoch 4/400: [==================================================] 1000/1000\n",
      "20s - loss: 0.0103 - val loss: 0.0094\n",
      "Epoch 5/400: [==================================================] 1000/1000\n",
      "20s - loss: 0.0104 - val loss: 0.0097\n",
      "Epoch 6/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0103 - val loss: 0.0087\n",
      "Epoch 7/400: [==================================================] 1000/1000\n",
      "20s - loss: 0.0094 - val loss: 0.0093\n",
      "Epoch 8/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0089 - val loss: 0.0083\n",
      "Epoch 9/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0085 - val loss: 0.0083\n",
      "Epoch 10/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0087 - val loss: 0.0086\n",
      "Epoch 11/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0079 - val loss: 0.0076\n",
      "Epoch 12/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0086 - val loss: 0.0087\n",
      "Epoch 13/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0079 - val loss: 0.0080\n",
      "Epoch 14/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0078 - val loss: 0.0073\n",
      "Epoch 15/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0080 - val loss: 0.0092\n",
      "Epoch 16/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0079 - val loss: 0.0071\n",
      "Epoch 17/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0074 - val loss: 0.0069\n",
      "Epoch 18/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0074 - val loss: 0.0066\n",
      "Epoch 19/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0078 - val loss: 0.0075\n",
      "Epoch 20/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0078 - val loss: 0.0161\n",
      "Epoch 21/400: [==================================================] 1000/1000\n",
      "18s - loss: 0.0079 - val loss: 0.0070\n",
      "Epoch 22/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0078 - val loss: 0.0071\n",
      "Epoch 23/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0071 - val loss: 0.0070\n",
      "Epoch 24/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0074 - val loss: 0.0067\n",
      "Epoch 25/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0071 - val loss: 0.0071\n",
      "Epoch 26/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0069 - val loss: 0.0064\n",
      "Epoch 27/400: [==================================================] 1000/1000\n",
      "18s - loss: 0.0066 - val loss: 0.0065\n",
      "Epoch 28/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0066 - val loss: 0.0067\n",
      "Epoch 29/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0068 - val loss: 0.0063\n",
      "Epoch 30/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0066 - val loss: 0.0064\n",
      "Epoch 31/400: [==================================================] 1000/1000\n",
      "17s - loss: 0.0066 - val loss: 0.0069\n",
      "Epoch 32/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0071 - val loss: 0.0065\n",
      "Epoch 33/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0067 - val loss: 0.0077\n",
      "Epoch 34/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0067 - val loss: 0.0078\n",
      "Epoch 35/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0069 - val loss: 0.0063\n",
      "Epoch 36/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0068 - val loss: 0.0072\n",
      "Epoch 37/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0066 - val loss: 0.0062\n",
      "Epoch 38/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0067 - val loss: 0.0065\n",
      "Epoch 39/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0065 - val loss: 0.0059\n",
      "Epoch 40/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0063 - val loss: 0.0062\n",
      "Epoch 41/400: [==================================================] 1000/1000\n",
      "18s - loss: 0.0062 - val loss: 0.0063\n",
      "Epoch 42/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0066 - val loss: 0.0071\n",
      "Epoch 43/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0070 - val loss: 0.0066\n",
      "Epoch 44/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0068 - val loss: 0.0067\n",
      "Epoch 45/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0068 - val loss: 0.0069\n",
      "Epoch 46/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0071 - val loss: 0.0063\n",
      "Epoch 47/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0069 - val loss: 0.0077\n",
      "Epoch 48/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0069 - val loss: 0.0073\n",
      "Epoch 49/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0068 - val loss: 0.0069\n",
      "Epoch 50/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0069 - val loss: 0.0071\n",
      "Epoch 51/400: [==================================================] 1000/1000\n",
      "18s - loss: 0.0068 - val loss: 0.0063\n",
      "Epoch 52/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0065 - val loss: 0.0063\n",
      "Epoch 53/400: [==================================================] 1000/1000\n",
      "18s - loss: 0.0066 - val loss: 0.0071\n",
      "Epoch 54/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0066 - val loss: 0.0066\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 55/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0063\n",
      "Epoch 56/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0064 - val loss: 0.0063\n",
      "Epoch 57/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0062\n",
      "Epoch 58/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0064 - val loss: 0.0059\n",
      "Epoch 59/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0060\n",
      "Epoch 60/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0067 - val loss: 0.0064\n",
      "Epoch 61/400: [==================================================] 1000/1000\n",
      "18s - loss: 0.0072 - val loss: 0.0068\n",
      "Epoch 62/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0065 - val loss: 0.0063\n",
      "Epoch 63/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0066 - val loss: 0.0070\n",
      "Epoch 64/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0067 - val loss: 0.0067\n",
      "Epoch 65/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0065 - val loss: 0.0063\n",
      "Epoch 66/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0063 - val loss: 0.0066\n",
      "Epoch 67/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0065 - val loss: 0.0064\n",
      "Epoch 68/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0064 - val loss: 0.0066\n",
      "Epoch 69/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0059\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 70/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0064 - val loss: 0.0068\n",
      "Epoch 71/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0064 - val loss: 0.0062\n",
      "Epoch 72/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0064 - val loss: 0.0073\n",
      "Epoch 73/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0058\n",
      "Epoch 74/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0063 - val loss: 0.0059\n",
      "Epoch 75/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0068\n",
      "Epoch 76/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0066 - val loss: 0.0068\n",
      "Epoch 77/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0066 - val loss: 0.0065\n",
      "Epoch 78/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0066 - val loss: 0.0060\n",
      "Epoch 79/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0065 - val loss: 0.0060\n",
      "Epoch 80/400: [==================================================] 1000/1000\n",
      "18s - loss: 0.0062 - val loss: 0.0072\n",
      "Epoch 81/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0064 - val loss: 0.0061\n",
      "Epoch 82/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0063 - val loss: 0.0066\n",
      "Epoch 83/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0064 - val loss: 0.0064\n",
      "Epoch 84/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0065 - val loss: 0.0057\n",
      "Epoch 85/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0060\n",
      "Epoch 86/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0065 - val loss: 0.0059\n",
      "Epoch 87/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0061 - val loss: 0.0058\n",
      "Epoch 88/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0064 - val loss: 0.0060\n",
      "Epoch 89/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0060\n",
      "Epoch 90/400: [==================================================] 1000/1000\n",
      "18s - loss: 0.0062 - val loss: 0.0061\n",
      "Epoch 91/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0063 - val loss: 0.0067\n",
      "Epoch 92/400: [==================================================] 1000/1000\n",
      "20s - loss: 0.0062 - val loss: 0.0065\n",
      "Epoch 93/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0065 - val loss: 0.0060\n",
      "Epoch 94/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0061\n",
      "Epoch 95/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0057\n",
      "Epoch 96/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0061 - val loss: 0.0068\n",
      "Epoch 97/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0063 - val loss: 0.0059\n",
      "Epoch 98/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0064 - val loss: 0.0058\n",
      "Epoch 99/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0061 - val loss: 0.0058\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 100/400: [==================================================] 1000/1000\n",
      "18s - loss: 0.0061 - val loss: 0.0063\n",
      "Epoch 101/400: [==================================================] 1000/1000\n",
      "18s - loss: 0.0062 - val loss: 0.0059\n",
      "Epoch 102/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0080\n",
      "Epoch 103/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0059\n",
      "Epoch 104/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0063 - val loss: 0.0057\n",
      "Epoch 105/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0063 - val loss: 0.0062\n",
      "Epoch 106/400: [==================================================] 1000/1000\n",
      "18s - loss: 0.0063 - val loss: 0.0077\n",
      "Epoch 107/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0059\n",
      "Epoch 108/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0063\n",
      "Epoch 109/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0062 - val loss: 0.0060\n",
      "Epoch 110/400: [==================================================] 1000/1000\n",
      "17s - loss: 0.0062 - val loss: 0.0070\n",
      "Epoch 111/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0065 - val loss: 0.0063\n",
      "Epoch 112/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0061 - val loss: 0.0063\n",
      "Epoch 113/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0061 - val loss: 0.0058\n",
      "Epoch 114/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0060 - val loss: 0.0062\n",
      "\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 115/400: [==================================================] 1000/1000\n",
      "19s - loss: 0.0065 - val loss: 0.0065\n",
      "Epoch 116/400: [========..........................................] 170/1000\r"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    if normalizer[0] in ['log', 'std']:\n",
    "        regression_loss = keras.losses.MeanSquaredError()\n",
    "    else:\n",
    "        def regression_loss(y_true, y_pred):\n",
    "            y_true /= normalizer[1]\n",
    "            return K.mean(K.square((y_true - y_pred) / (y_true + K.epsilon())), axis=-1)\n",
    "\n",
    "    classification_loss = keras.losses.BinaryCrossentropy()\n",
    "    \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001)\n",
    "    losses = {'regression': regression_loss, 'classification': classification_loss}\n",
    "    loss_weights = {'regression': 0.99, 'classification': 0.01}\n",
    "        \n",
    "    model.compile(optimizer=optimizer, loss=losses, loss_weights=loss_weights, metrics=['accuracy'])\n",
    "    \n",
    "    callbacks = [PrinterCallback(), \n",
    "                 keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=15, min_lr=0.0000001, verbose=1),\n",
    "                 keras.callbacks.EarlyStopping(patience=40, restore_best_weights=True, verbose=1)]\n",
    "    \n",
    "    history = model.fit(train_generator.generator(), \n",
    "                        validation_data=validation_generator.generator(),\n",
    "                        steps_per_epoch=1000,\n",
    "                        validation_steps=250,\n",
    "                        shuffle=True,\n",
    "                        epochs=400,\n",
    "                        callbacks=[callbacks],\n",
    "                        verbose=0)\n",
    "    history = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a1a6c-66e1-413c-a999-5853d612acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save:\n",
    "    if train:\n",
    "        model.save(out_path + f'models/{q}GarNet_{normalizer[0]}_{vmax}')\n",
    "\n",
    "        with open(out_path + f'models/{q}GarNet_{normalizer[0]}_{vmax}/history.pickle', 'wb') as handle:\n",
    "            pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(out_path + f'models/{q}GarNet_{normalizer[0]}_{vmax}/scaler.pickle', 'wb') as handle:\n",
    "            pickle.dump(scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    K.clear_session()\n",
    "    model = tf.keras.models.load_model(out_path + f'models/{q}GarNet_{normalizer[0]}_{vmax}', \n",
    "                                   custom_objects={\"GarNetModel\": GarNetModel}, compile=False)\n",
    "\n",
    "    with open(out_path + f'models/{q}GarNet_{normalizer[0]}_{vmax}/history.pickle', \"rb\") as file:\n",
    "        history = pickle.load(file)\n",
    "    \n",
    "    with open(out_path + f'models/{q}GarNet_{normalizer[0]}_{vmax}/scaler.pickle', \"rb\") as file:\n",
    "        scaler = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bba9ae-cd7e-446d-a779-097776f2b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(test_generator.generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19377a68-67d8-4c02-88a2-8021926eefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d78aa-6a5d-4202-95d4-270f14eb3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(5,5))\n",
    "# ax = plt.axes(projection='3d')\n",
    "# data = x[0][24]\n",
    "# data = data[data[:,3]!=0].reshape(-1,4)\n",
    "# p = ax.scatter3D(data[:,2], data[:,0], data[:,1], s=64, c=data[:,3], cmap='inferno');\n",
    "# ax.grid(False)\n",
    "# ax.xaxis.set_ticklabels([])\n",
    "# ax.yaxis.set_ticklabels([])\n",
    "# ax.zaxis.set_ticklabels([])\n",
    "\n",
    "# for line in ax.xaxis.get_ticklines():\n",
    "#     line.set_visible(False)\n",
    "# for line in ax.yaxis.get_ticklines():\n",
    "#     line.set_visible(False)\n",
    "# for line in ax.zaxis.get_ticklines():\n",
    "#     line.set_visible(False)\n",
    "# ax.xaxis.pane.fill = False\n",
    "# ax.yaxis.pane.fill = False\n",
    "# ax.zaxis.pane.fill = False\n",
    "# ax.xaxis.pane.set_edgecolor('w')\n",
    "# ax.yaxis.pane.set_edgecolor('w')\n",
    "# ax.zaxis.pane.set_edgecolor('w')\n",
    "# ax.set_xlabel('Samp.', labelpad=-10)\n",
    "# ax.set_ylabel(r'$\\eta$', labelpad=-10)\n",
    "# ax.set_zlabel(r'$\\phi$', labelpad=-10)\n",
    "# ax.set_title('Charged Pion Cluster', pad=-40)\n",
    "# ax.zaxis.set_rotate_label(False)\n",
    "# ax.yaxis.set_rotate_label(False)\n",
    "# ax.xaxis.set_rotate_label(False)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7f36c-1411-40f6-901f-86bfd70ee886",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_curve = Plotter(training, history=history, metrics=['loss'], scale='log')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1d0a8-75fa-4027-8f4d-9a4ed87841ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_curve = Plotter(training, history=history, metrics=['regression_accuracy'])\n",
    "accuracy_curve.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48c2e1-c86e-415d-9bae-a35722d6a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = Plotter(roc, preds=[model.predict(x)[0][:,0]], targets=[y['classification'][:,0]], labels=[''])\n",
    "ROC.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381290cf-0fad-4f47-bec3-ed4a589e92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalizer[0] == 'log':\n",
    "    fit_pred = np.exp(model.predict(x)[-1] * 10).reshape(-1,)\n",
    "    fit_target = np.exp(y['regression'] * 10).reshape(-1,)\n",
    "elif normalizer[0] == 'std':\n",
    "    scaler = normalizer[1]\n",
    "    fit_pred = scaler.inverse_transform(model.predict(x)[-1]).reshape(-1,)\n",
    "    fit_target = scaler.inverse_transform(np.reshape(y['regression'], (-1, 1))).reshape(-1,)\n",
    "elif normalizer[0] == 'max':\n",
    "    fit_pred = model.predict(x)[-1].reshape(-1,) * normalizer[1]\n",
    "    fit_target = y['regression'].reshape(-1,) * normalizer[1]\n",
    "else:\n",
    "    fit_pred = model.predict(x)[-1].reshape(-1,)\n",
    "    fit_target = y['regression'].reshape(-1,)\n",
    "\n",
    "reg = Plotter(regResponse, \n",
    "              pred=fit_pred, \n",
    "              target=fit_target,\n",
    "              stat=['median'],\n",
    "              title='Continuous Keras Model Regression Response')\n",
    "reg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df041e-6986-438e-bc68-8d75136f748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = Plotter(regResponseOverlay,\n",
    "#               preds=[orig_pred, fit_pred],\n",
    "#               targets=[orig_target, fit_target],\n",
    "#               labels=['Pre-Fit', 'Post-Fit'],\n",
    "#               stat=['mean', 'stdmean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e9314-cb61-4ab0-907e-f180cb9b7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = tf.keras.models.load_model(out_path + f'models/GarNet_log_64', \n",
    "                                   custom_objects={\"GarNetModel\": GarNetModel}, compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b08861-d46e-4f11-959d-d4e4f1722f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_path + f'models/qGarNet_log_32/history.pickle', \"rb\") as file:\n",
    "        history = pickle.load(file)\n",
    "        \n",
    "loss_curve = Plotter(training, history=history, metrics=['loss'], scale='log')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed916f26-2d41-4ec0-8773-097ab8c6b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []\n",
    "preds = []\n",
    "targets = []\n",
    "labels = []\n",
    "for v in [32, 64]:\n",
    "    name = f'garnet_log_{v}'\n",
    "    test_generator = garnetDataGenerator(test_file_list,\n",
    "                                         cell_geo_path,\n",
    "                                         batch_size=20000,\n",
    "                                         normalizer=normalizer,\n",
    "                                         name=name,\n",
    "                                         vmax=v,\n",
    "                                         labeled=True,\n",
    "                                         preprocess=False,\n",
    "                                         output_dir=out_path + 'test/',\n",
    "                                         data_format=data_format,\n",
    "                                         filterfunc=filterfunction)\n",
    "    x, y = next(test_generator.generator())\n",
    "    for q in ['q']:\n",
    "        model = tf.keras.models.load_model(out_path + f'models/{q}GarNet_log_{v}', \n",
    "                                           custom_objects={\"GarNetModel\": GarNetModel}, compile=False)\n",
    "        preds.append(np.exp(model.predict(x)[-1] * 10).reshape(-1,))\n",
    "        targets.append(np.exp(y['regression'] * 10).reshape(-1,))\n",
    "        \n",
    "        histories.append(pickle.load(open(out_path + f'models/{q}GarNet_log_{v}/history.pickle', \"rb\")))\n",
    "        if q=='q':\n",
    "            labels.append(f'{v} Cell Quantized')\n",
    "        else:\n",
    "            labels.append(f'{v} Cell Continuous')\n",
    "        \n",
    "loss_curve = Plotter(trainingOverlay, histories=histories, labels=labels, metric='loss', scale='log')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9305d06-ee02-4beb-821d-98b140fc64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Plotter(regResponseOverlay,\n",
    "              preds=preds,\n",
    "              targets=targets,\n",
    "              labels=labels,\n",
    "              stat=['median'])\n",
    "reg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be2726-70b1-469c-8011-4abb9ee9808f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
